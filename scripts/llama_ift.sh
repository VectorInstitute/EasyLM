python -m EasyLM.models.llama.llama_train \
    --mp_mesh_dim='8,1' \
    --total_steps=20000 \
    --log_freq=400 \
    --save_milestone_freq=20000 \
    --eval_steps=160 \
    --load_llama_config='13b' \
    --update_llama_config='{"resid_pdrop": 0.05, "embd_pdrop": 0.05, "attn_pdrop": 0.05, "fcm_max_ratio": 0.1}' \
    --load_checkpoint='params::data/models/llama-13b' \
    --tokenizer.vocab_file='data/models/llama/tokenizer.model' \
    --optimizer.type='adamw' \
    --optimizer.accumulate_gradient_steps=16 \
    --optimizer.adamw_optimizer.weight_decay=0.001 \
    --optimizer.adamw_optimizer.lr=0.002 \
    --optimizer.adamw_optimizer.end_lr=0.002 \
    --optimizer.adamw_optimizer.lr_warmup_steps=1000 \
    --optimizer.adamw_optimizer.lr_decay_steps=100000000 \
    --optimizer.adamw_optimizer.multiply_by_parameter_scale=True \
    --optimizer.adamw_optimizer.bf16_momentum=True \
    --train_dataset.type='json' \
    --train_dataset.text_processor.fields='[instruction],output' \
    --train_dataset.text_processor.prepend_text='' \
    --train_dataset.json_dataset.path='data/easy-lm/processed/alpaca_train.json' \
    --train_dataset.json_dataset.seq_length=2048 \
    --train_dataset.json_dataset.batch_size=2 \
    --eval_dataset.type='json' \
    --eval_dataset.text_processor.fields='[instruction],output' \
    --eval_dataset.text_processor.prepend_text='' \
    --eval_dataset.json_dataset.path='data/easy-lm/processed/alpaca_validation.json' \
    --eval_dataset.json_dataset.seq_length=2048 \
    --eval_dataset.json_dataset.batch_size=2 \
    --logger.online=True \
    --logger.project="Koala" \
    --logger.output_dir="data/easy-lm/logs" \
    --logger.notes="llama13b_alpaca_no_decoration" \